---
title: "Nöral Makine Çeviri Modelini Görselleştirme (Attention Mekanizması ile Seq2seq Modeli)"
date: 2018-09-24
tags: [nlp,ml, seq2seq]
header:
  image: ""
excerpt: "nlp, ml, seq2seq"
mathjax: "true"
---

<p align="justify" >Bu makale, [Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) Türkçe çevirisidir.Kaynak, aslında MIT sınıflarında kullanılan Jay Alammar [@JayAlammar](https://twitter.com/JayAlammar) tarafından yazılmış bir blog makalesidir.</p>


<p align="justify" >Sequence-to-sequence (kısaca seq2seq) modeller, makine çevirisi, metin özetleme ve görüntü yazısı oluşturma gibi görevlerde çok sayıda başarı elde eden derin öğrenme modelleridir. Google Translate, 2016'nın sonlarında böyle bir [modeli](https://blog.google/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/) üretimde kullanmaya başladı. Bu modeller, iki öncü makalede açıklanmıştır [Sutskever ve diğerleri, 2014](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf), [Cho ve diğerleri, 2014](http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf).</p>

<p align="justify" > Bununla birlikte, modeli tam olarak anlamak ve uygulamak için, birbirine dayalı bir dizi kavramın çözülmesi gerektiği ortaya çıktı. Bu fikirlerin görsel olarak ifade edilirse daha tanıdık geleceğini düşündüm. Bu makalenin amacı budur. Bu makaleyi okumak için daha önce açıklanan derin öğrenmeyi anlamanız gerekir. Yukarıdaki makaleleri (ve gönderide daha sonra bağlantı verilen öne çıkan makaleler) okurken yararlı bulacağınızı umuyoruz.</p> 

<p align="justify" >Seq2seq model, bir dizi öğeyi alan (kelimeler, harfler, bir görüntünün özellikleri … vb.) ve başka bir dizi döndüren bir modeldir. Eğitimli bir model şu şekilde çalışır:</p>

<img src="gifs/seq2seq_1.gif" alt="seq2seq_1.gif">

<p align="justify" >Nöral makine çevirisinde, dizi birbiri ardına işlenen bir grup kelimedir. Aynı şekilde çıktı da bir grup kelimedir:</p>

<img src="gifs/seq2seq_2.gif" alt="seq2seq_2.gif">

### Daha yakından bakalım

<p align="justify" >Seq2seq model, bir kodlayıcı (encoder) ve bir kod çözücüden (decoder) oluşur. Kodlayıcı, girdi dizisindeki her bir öğeyi işler ve yakalanan bilgileri “bağlam” (context) adı verilen bir vektörde derler. Kodlayıcı tüm girdi dizisini işledikten sonra , içeriği kod çözücüye gönderir ve bu da çıktı dizisinin her bir öğesini üretir.</p>

<img src="gifs/seq2seq_3.gif" alt="seq2seq_3.gif">

Aynı işlem makine çevirisi için de geçerlidir.

<img src="gifs/seq2seq_4.gif" alt="seq2seq_4.gif">

<p align="justify" >Makine çevirisi durumunda bağlam , temelde bir sayı dizisinden oluşan bir vektördür. Kodlayıcı ve kod çözücünün her ikisi de RNN olma eğilimindedir.(RNN’lere giriş için Luis Serrano’tan [A friendly introduction to Recurrent Neural Networks](https://www.youtube.com/watch?v=UNmqTiOnRfg) ‘a göz atabilirsiniz).</p>

<img src="gifs/context.png" alt="context.png">

Bağlam, bir float vektörüdür. İlerleyen bölümlerde, daha yüksek değerlere sahip hücrelere daha parlak renkler atanarak vektörler renk tonlarıyla temsil edilecektir.


<p align="justify" ></p>
<p align="justify" ></p>
<p align="justify" ></p>
<p align="justify" ></p>

And here's some *italics*

Here's some **bold** text.

What about a [link](https://github.com/dataoptimal)?

Here's a bulleted list:
* First item
+ Second item
- Third item

Here's a numbered list:
1. First
2. Second
3. Third

